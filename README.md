Challenge 1: Tokenization : You have a collection of tickets enclosed below. Apply tokenization using python nltk and generate a vocabulary of words in the ticket. 

Challenge 2 :  Tokenization, FDIST, ngrams This is an extension to the Challenge 1  .  Once you have tokenized, find the n-grams using FDIST function. FDIST and ngrams will give you the laundry list but has a lot of noise, hence the objective is to extract the key frequently occurring ngrams (where n>1 and n<=4) based on n=1 most frequently occurring terms. Assume that you need to be able to present to the Customer key issues found in the ticket database. Please feel free to explore the inclusion of STOPWORDS in this solution

Challenge 3 :  Assume the Customer has a collection of requirements that have been implemented in the past.  Customer is looking for a mechanism to identify if the new incoming requirement is a match to one of the implemented ones. The challenge is Customers organization spends quite a bit of time understanding if they have done it before. In the bigger picture Customer wants to leverage the learning of the past before implementing a similar requirement. Enclosed is a sample set of requirements, come up with an approach to identify the match against the incoming requirement, you need to select an appropriate measure to reflect the match percentage. You must include STOPWORDS in this solution.
